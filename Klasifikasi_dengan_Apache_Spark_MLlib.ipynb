{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBNHdZIBnZufRBni7PCH00",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyulianton/BigData/blob/main/Klasifikasi_dengan_Apache_Spark_MLlib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Contoh 1: Klasifikasi dengan Apache Spark MLlib (PySpark)\n",
        "\n",
        "**Tujuan:** Mendemonstrasikan bagaimana melakukan klasifikasi menggunakan Logistic Regression di Spark MLlib (PySpark) pada dataset Iris.\n",
        "\n",
        "**Dataset:** Iris Dataset (dataset klasik untuk klasifikasi, tersedia di scikit-learn)."
      ],
      "metadata": {
        "id": "mel1-I3KSfWk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hITgpdqvSKeg"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://downloads.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "-KLNpKMCSv00"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf spark-3.5.5-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "9-UKcon9S-Ih"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.5-bin-hadoop3\""
      ],
      "metadata": {
        "id": "ko1fn5kITFiR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark -q -q -q\n",
        "!pip install pyspark -q -q -q"
      ],
      "metadata": {
        "id": "PUCeIljKTRcq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Langkah 1: Inisialisasi SparkSession ---\n",
        "# SparkSession adalah entry point untuk memprogram Spark dengan Dataset dan DataFrame API.\n",
        "# Ini ibarat pintu gerbang ke cluster Spark kita.\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Membuat SparkSession.\n",
        "# .builder: Memulai konfigurasi SparkSession.\n",
        "# .appName(\"IrisClassification\"): Memberi nama aplikasi Spark kita.\n",
        "# .master(\"local[*]\"): Menentukan mode Spark. \"local[*]\" berarti Spark akan berjalan secara lokal\n",
        "#                      menggunakan semua core CPU yang tersedia. Untuk cluster, ini akan diganti\n",
        "#                      dengan URL master Spark (misal: \"spark://<master_ip>:7077\").\n",
        "# .getOrCreate(): Mengembalikan SparkSession yang sudah ada atau membuat yang baru.\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"IrisClassification\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"SparkSession berhasil diinisialisasi.\")\n",
        "print(f\"Spark UI available at: {spark.sparkContext.uiWebUrl}\")\n",
        "\n",
        "# --- Langkah 2: Muat Dataset (Simulasi Data Big Data) ---\n",
        "# Karena dataset Iris kecil, kita akan memuatnya melalui scikit-learn,\n",
        "# lalu mengkonversinya menjadi Spark DataFrame.\n",
        "# Dalam skenario Big Data sesungguhnya, data akan dimuat langsung dari HDFS, S3, dll.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Muat dataset Iris dari scikit-learn\n",
        "iris = load_iris()\n",
        "# Konversi ke Pandas DataFrame untuk kemudahan\n",
        "iris_df_pandas = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "# Tambahkan kolom target (label)\n",
        "iris_df_pandas['target'] = iris.target\n",
        "iris_df_pandas['target_name'] = iris.target_names[iris.target]\n",
        "\n",
        "print(\"\\nDataset Iris (Pandas DataFrame):\")\n",
        "print(iris_df_pandas.head())\n",
        "print(f\"Ukuran dataset: {iris_df_pandas.shape[0]} baris, {iris_df_pandas.shape[1]} kolom.\")\n",
        "\n",
        "# Konversi Pandas DataFrame ke Spark DataFrame\n",
        "# Ini adalah langkah krusial untuk membawa data ke ekosistem Spark\n",
        "iris_df_spark = spark.createDataFrame(iris_df_pandas)\n",
        "\n",
        "print(\"\\nDataset Iris (Spark DataFrame - Schema):\")\n",
        "iris_df_spark.printSchema()\n",
        "print(\"\\nDataset Iris (Spark DataFrame - Contoh Data):\")\n",
        "iris_df_spark.show(5)\n",
        "\n",
        "# --- Langkah 3: Persiapan Data untuk MLlib (Feature Engineering) ---\n",
        "# MLlib membutuhkan fitur-fitur (input untuk model) dalam satu kolom bertipe Vector.\n",
        "# Ini adalah konsep `VectorAssembler`.\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Tentukan kolom fitur (input untuk model) dan kolom target (label)\n",
        "feature_cols = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
        "label_col = 'target' # Menggunakan kolom target numerik\n",
        "\n",
        "# Inisialisasi VectorAssembler:\n",
        "# inputCols: Daftar kolom yang akan digabungkan menjadi satu vektor fitur.\n",
        "# outputCol: Nama kolom baru yang akan berisi vektor fitur.\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "# Terapkan transformasi assembler ke Spark DataFrame\n",
        "# Ini akan menghasilkan DataFrame baru dengan kolom 'features' yang berisi vektor fitur.\n",
        "data = assembler.transform(iris_df_spark)\n",
        "\n",
        "print(\"\\nSpark DataFrame setelah VectorAssembler (kolom 'features' ditambahkan):\")\n",
        "data.select(\"features\", label_col).show(5, truncate=False) # truncate=False agar vektor fitur tidak terpotong\n",
        "\n",
        "# --- Langkah 4: Pembagian Data (Training dan Testing) ---\n",
        "# Kita perlu membagi data menjadi set pelatihan (training) dan set pengujian (testing)\n",
        "# untuk mengevaluasi kinerja model.\n",
        "\n",
        "# randomSplit: Membagi DataFrame secara acak berdasarkan bobot yang diberikan.\n",
        "# [0.7, 0.3]: 70% untuk training, 30% untuk testing.\n",
        "# seed: Untuk reproduksibilitas (agar hasil pembagian selalu sama).\n",
        "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "print(f\"\\nUkuran data training: {train_data.count()} baris.\")\n",
        "print(f\"Ukuran data testing: {test_data.count()} baris.\")\n",
        "\n",
        "# --- Langkah 5: Membangun dan Melatih Model Klasifikasi (Logistic Regression) ---\n",
        "# Menggunakan algoritma Logistic Regression dari MLlib.\n",
        "\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Inisialisasi model LogisticRegression:\n",
        "# labelCol: Nama kolom yang berisi label (target).\n",
        "# featuresCol: Nama kolom yang berisi vektor fitur (hasil dari VectorAssembler).\n",
        "# family: Tipe klasifikasi ('binomial' untuk 2 kelas, 'multinomial' untuk >2 kelas).\n",
        "lr = LogisticRegression(labelCol=label_col, featuresCol=\"features\", family=\"multinomial\")\n",
        "\n",
        "print(\"\\nMemulai pelatihan model Logistic Regression...\")\n",
        "# Melatih model menggunakan data training. Ini adalah langkah komputasi utama yang terdistribusi.\n",
        "lr_model = lr.fit(train_data)\n",
        "print(\"Pelatihan model selesai.\")\n",
        "\n",
        "# --- Langkah 6: Evaluasi Model ---\n",
        "# Menggunakan model yang sudah dilatih untuk membuat prediksi pada data testing\n",
        "# dan mengevaluasi seberapa baik kinerja model.\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Melakukan prediksi pada data testing.\n",
        "# Model akan menambahkan kolom 'prediction' dan 'probability' ke DataFrame.\n",
        "predictions = lr_model.transform(test_data)\n",
        "\n",
        "print(\"\\nPrediksi pada data testing (kolom 'prediction' dan 'probability' ditambahkan):\")\n",
        "predictions.select(label_col, \"prediction\", \"probability\").show(5, truncate=False)\n",
        "\n",
        "# Inisialisasi evaluator untuk klasifikasi multi-kelas\n",
        "# labelCol: Nama kolom label sebenarnya.\n",
        "# predictionCol: Nama kolom hasil prediksi model.\n",
        "# metricName: Metrik evaluasi yang akan digunakan ('accuracy' adalah umum).\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# Hitung akurasi model\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"\\nAkurasiasi Model pada data testing: {accuracy:.4f}\")\n",
        "\n",
        "# --- Langkah 7: Menghentikan SparkSession ---\n",
        "# Penting untuk menghentikan SparkSession setelah selesai menggunakan sumber daya.\n",
        "spark.stop()\n",
        "print(\"\\nSparkSession dihentikan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3liDxXK1TdFS",
        "outputId": "4529e0e7-1c86-41fd-e26d-8beab30bb1bb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkSession berhasil diinisialisasi.\n",
            "Spark UI available at: http://a7c41d6b328d:4040\n",
            "\n",
            "Dataset Iris (Pandas DataFrame):\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                4.7               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "\n",
            "   target target_name  \n",
            "0       0      setosa  \n",
            "1       0      setosa  \n",
            "2       0      setosa  \n",
            "3       0      setosa  \n",
            "4       0      setosa  \n",
            "Ukuran dataset: 150 baris, 6 kolom.\n",
            "\n",
            "Dataset Iris (Spark DataFrame - Schema):\n",
            "root\n",
            " |-- sepal length (cm): double (nullable = true)\n",
            " |-- sepal width (cm): double (nullable = true)\n",
            " |-- petal length (cm): double (nullable = true)\n",
            " |-- petal width (cm): double (nullable = true)\n",
            " |-- target: long (nullable = true)\n",
            " |-- target_name: string (nullable = true)\n",
            "\n",
            "\n",
            "Dataset Iris (Spark DataFrame - Contoh Data):\n",
            "+-----------------+----------------+-----------------+----------------+------+-----------+\n",
            "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|target|target_name|\n",
            "+-----------------+----------------+-----------------+----------------+------+-----------+\n",
            "|              5.1|             3.5|              1.4|             0.2|     0|     setosa|\n",
            "|              4.9|             3.0|              1.4|             0.2|     0|     setosa|\n",
            "|              4.7|             3.2|              1.3|             0.2|     0|     setosa|\n",
            "|              4.6|             3.1|              1.5|             0.2|     0|     setosa|\n",
            "|              5.0|             3.6|              1.4|             0.2|     0|     setosa|\n",
            "+-----------------+----------------+-----------------+----------------+------+-----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Spark DataFrame setelah VectorAssembler (kolom 'features' ditambahkan):\n",
            "+-----------------+------+\n",
            "|features         |target|\n",
            "+-----------------+------+\n",
            "|[5.1,3.5,1.4,0.2]|0     |\n",
            "|[4.9,3.0,1.4,0.2]|0     |\n",
            "|[4.7,3.2,1.3,0.2]|0     |\n",
            "|[4.6,3.1,1.5,0.2]|0     |\n",
            "|[5.0,3.6,1.4,0.2]|0     |\n",
            "+-----------------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Ukuran data training: 94 baris.\n",
            "Ukuran data testing: 56 baris.\n",
            "\n",
            "Memulai pelatihan model Logistic Regression...\n",
            "Pelatihan model selesai.\n",
            "\n",
            "Prediksi pada data testing (kolom 'prediction' dan 'probability' ditambahkan):\n",
            "+------+----------+---------------------------------------------------+\n",
            "|target|prediction|probability                                        |\n",
            "+------+----------+---------------------------------------------------+\n",
            "|0     |0.0       |[1.0,1.1008480267387954E-29,4.08876269200743E-55]  |\n",
            "|0     |0.0       |[1.0,2.8385538334625506E-32,7.127785014417726E-58] |\n",
            "|0     |0.0       |[1.0,7.220740773596769E-43,3.1284922015693614E-71] |\n",
            "|0     |0.0       |[1.0,1.8250783149280925E-32,1.6080533492544765E-58]|\n",
            "|0     |0.0       |[1.0,3.7449838628079537E-28,1.0644851971389475E-52]|\n",
            "+------+----------+---------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Akurasiasi Model pada data testing: 0.9643\n",
            "\n",
            "SparkSession dihentikan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Penjelasan Alur Cerita Program Spark MLlib:**\n",
        "\n",
        "1.  **Inisialisasi SparkSession:** Ini seperti \"menghidupkan\" mesin pabrik Big Data kita. Tanpa ini, tidak ada yang bisa kita lakukan. `local[*]` berarti kita menggunakan semua inti CPU di mesin lokal kita sebagai \"cluster\" mini.\n",
        "2.  **Muat Dataset:** Kita mengambil \"bahan baku\" (data Iris) dan mengubahnya menjadi format yang bisa dipahami dan diproses oleh Spark (Spark DataFrame). Dalam skenario nyata, ini akan melibatkan membaca data dari lokasi terdistribusi seperti HDFS atau S3.\n",
        "3.  **Persiapan Data (Feature Engineering):** Ini adalah tahap \"merakit\" bahan baku menjadi bentuk yang bisa \"dicerna\" oleh algoritma ML. `VectorAssembler` mengambil kolom-kolom fitur individual dan menggabungkannya menjadi satu kolom vektor, yang merupakan format standar yang dibutuhkan oleh MLlib.\n",
        "4.  **Pembagian Data:** Kita memisahkan sebagian data untuk \"pelatihan\" model (data _training_) dan sebagian lain untuk \"menguji\" seberapa baik model kita bekerja pada data yang belum pernah dilihatnya (data _testing_). Ini penting untuk menghindari _overfitting_.\n",
        "5.  **Membangun & Melatih Model:** Di sinilah \"otak\" pabrik bekerja. Kita memilih algoritma `LogisticRegression` dan \"mengajarkan\" model menggunakan `train_data`. Seluruh proses pelatihan ini secara otomatis didistribusikan oleh Spark ke seluruh _node_ (atau _core_ lokal kita).\n",
        "6.  **Evaluasi Model:** Setelah model dilatih, kita menguji kemampuannya. Kita membiarkan model membuat prediksi pada `test_data`, lalu membandingkan prediksi itu dengan label sebenarnya untuk melihat seberapa akurat model kita.\n",
        "7.  **Menghentikan SparkSession:** Setelah pekerjaan selesai, kita \"mematikan\" mesin pabrik untuk membebaskan sumber daya.\n"
      ],
      "metadata": {
        "id": "kSmDTGJAT0zC"
      }
    }
  ]
}